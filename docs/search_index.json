[
["descriptive-spatio-temporal-statistical-models.html", "Chapter 4 Descriptive Spatio-Temporal Statistical Models 4.1 Additive Measurement Error and Process models 4.2 prediction for Gaussian Data and Processes. 4.3 Random-Effects Parameterizations 4.4 Basis-Function Representations", " Chapter 4 Descriptive Spatio-Temporal Statistical Models We saw that there are two approaches for spatio-temporal statistics, which is two D’s. One is descriptive approach which will be covered in this chapter. The other is dynamic approach which will be introduced in the next chapter. Our main goals are 1) to predict unknown value at some location and time point, 2) to estimate spatio-temporal covariates. For both goals, we assume the our observations can be decomposed as observations = true process + observation error true process = regression component + dependent random process For descriptive approach, our main concern is to specify the dependence structure in the (dependent) random process whereas dynamic approach is mainly interested in the evolution of the dependent random process through time. 4.1 Additive Measurement Error and Process models In this section, we consider two stage model, which consists of data (observation) model and a process model that is again consists of fixed effect and random effect. This general decomposition is the basis for the models that we present in this and next chapter. Recall that for eath time point \\(t \\in \\{t_1, \\cdots, t_T \\}\\), we assume \\(m_{t_j}\\) observations. That is, our observation is \\[\\begin{equation} \\mathbb{Z} = Z(s_{11}; t_1), Z(s_{21}; t_1), \\cdots Z(s_{m_1 1}; t_1), \\cdots, Z(s_{1T}; t_T), \\cdots, Z(s_{m_t T}; t_T) \\end{equation}\\] In the situation where we want to predict values at spatio-temporal location \\((s_0; t_0)\\), if \\(t_0 &gt; t_T\\), we are in a forecasting situation and if \\(t_0 &lt; t_T\\), we have all data avilable to us, so we are in a smoothing situation. We assume that there is an underlying latent random spatio-temporal process which is denoted by \\(Y(s;t): s\\in D_s, t\\in D_t\\). However, what we see from the data is the noisy version of this latent random process, which is denoted by \\[\\begin{equation} Z(s_{ij}; t_j) = Y(s_{ij}; t_j) + \\epsilon(s_{ij}; t_j) \\tag{1} \\end{equation}\\] where \\(\\epsilon(s_{ij}; t_j)\\) represent iid mean-zero measurement error that is independent of \\(Y\\) and has variance \\(\\sigma^2_\\epsilon\\). Now we assume that the latent process model follows another model as \\[\\begin{equation} Y(s;t) = \\mu(s;t) + \\eta(s;t) \\tag{2} \\end{equation}\\] In (2), \\(\\mu(s;t)\\) represents the process mean which is not random and \\(\\eta(s;t)\\) represents a mean-zero random process with spatial and temporal statistical dependence. Depending on the problem, we may choose \\(\\mu(s;t)\\) as 1) known, 2) constant but unknown, 3) modeled in terms of \\(p\\) covariates, i.e., \\(\\mu(s;t) = x(s;t)&#39;\\beta\\), which is called respectively as 1) simple, 2) ordinary, 3) universal kriging. 4.2 prediction for Gaussian Data and Processes. In chapter 2, recall that we consider some deterministic weight, which is IDW, that smoothes surrounding datat points. In making the IDW, no statistical optimal methods are used, but cross validation for determining \\(\\alpha\\) is used. Now, we consider a optimization criterion, which is $E(Y(s_0; t_0) - (s_0; t_0))^2 $, the mean square prediction error (MPSE). The best linear unbiased predictor that minimizes the MSPE is referred as the kriging predictor. To determine optimal linear predictor, which we henceforth call S-T kriging, it is convenient to assume that the underlying latent process is a Gaussian process and the measurement error has a Gaussian distribution. In vector representation, \\[\\begin{equation} \\mathbf{Z} = \\mathbf{Y} + \\epsilon \\\\ \\mathbf{Y} = \\mathbb{\\mu} + \\mathbb{\\eta} \\tag{3} \\end{equation}\\] where we assume universal kriging, i.e., \\(\\mathbb{\\mu} = \\mathbf{X} \\mathbb{\\beta}\\). Also, \\(Cov(\\mathbf{Y}) \\equiv \\mathbf{C}_y = \\mathbf{C}_\\eta\\), \\(Cov(\\epsilon) \\equiv \\mathbf{C}_\\epsilon\\), \\(Cov(\\mathbf{Z}) \\equiv \\mathbf{C}_z = \\mathbf{C}_y + \\mathbf{C}_\\epsilon\\). For \\((s_0, t_0)\\), consider the joint Gaussian distribution, \\[\\begin{equation} \\left[ {\\begin{array}{cc} Y(s_0; t_0) \\\\ \\mathbf{Z} \\\\ \\end{array} } \\right] \\sim Gau \\left( \\left[ {\\begin{array}{cc} Y(s_0; t_0) \\\\ \\mathbf{Z} \\\\ \\end{array} } \\right] \\mathbf{\\beta}, \\left[ {\\begin{array}{cc} c_{0,0} &amp; \\mathbf{c}^{&#39;}_0 \\\\ \\mathbf{c}_0 &amp; \\mathbf{C}_z \\\\ \\end{array} } \\right] \\right) \\tag{4} \\end{equation}\\] By the property of Gaussian distribution, the conditional distribution can be derived easily, \\[\\begin{equation} Y(s_0; t_0) \\mid \\mathbf{Z} \\sim Gau( mean, var) \\tag{5} \\end{equation}\\] where the conditional mean in (5) is S-T simple kriging predictor, \\[\\begin{equation} \\hat{Y}(s_0; t_0) = \\mathbf{x}(s_0; t_0)&#39; \\beta + \\mathbf{c}&#39;_0 \\mathbf{C}^{-1}_z (\\mathbf{Z} - \\mathbf{X\\beta}) \\tag{6} \\end{equation}\\] and the variance is the conditional variance in (5) \\[\\begin{equation} \\sigma^2_{Y, sk}(s_0; t_0) = c_{0,0} - \\mathbf{c}&#39;_0 \\mathbf{C}^{-1}_z \\mathbf{c}_0 \\tag{7} \\end{equation}\\] Note that for any location, we can get S-T simple kriging predictor. This means that we assume that the process is defined for an uncountable set of locations and the data correspond to a partial realization of this process. This is also a property of Gaussian process. Another note is that from (6), the S-T simple kriging predictor is the combiation of original mean and the residual, where the weight is \\(\\mathbf{w} \\equiv \\mathbf{c}&#39;_0 \\mathbf{C}^{-1}_z\\). This can be views as the trend term \\(\\mathbf{x}(s_0; t_0)&#39;\\beta\\) is the mean of \\(Y(s_0; t_0)\\) prior to considering the observations. Note that \\(\\beta\\) is unknown parameter, which is to be estimated. Replacing it with the gls estimator, we get the optimal linear unbiased predictor, or S-T universal kriging predictor of \\(Y(s_0; t_0)\\) as \\[\\begin{equation} \\hat{Y}(s_0; t_0) = \\mathbf{x}(s_0; t_0)&#39; \\hat{\\beta}_{gls} + \\mathbf{c}&#39;_0 \\mathbf{C}^{-1}_z (\\mathbf{Z} - \\mathbf{X\\hat{\\beta}_{gls}}) \\tag{8} \\end{equation}\\] where \\[\\begin{equation} \\hat{\\beta}_{gls} \\equiv (\\mathbf{X}&#39; \\mathbf{C}^{-1}_z \\mathbf{X})^{-1} \\mathbf{X}&#39; \\mathbf{C}^{-1}_z \\mathbf{Z} \\tag{9} \\end{equation}\\] with \\[\\begin{equation} \\sigma^2_{Y, sk}(s_0; t_0) = c_{0,0} - \\mathbf{c}&#39;_0 \\mathbf{C}^{-1}_z \\mathbf{c}_0 + \\kappa \\tag{10} \\end{equation}\\] which is prediction standard error. For those of you who learn the spatial statistics before, the form of kriging estimator and variance is vary similar of that of spatio-temporal estimator. In practice, we also do not know the covariance function, \\(\\mathbf{C}_z, \\mathbf{c}_0\\), so we need to assume the structure of covariance function or parameterize to estimate them. Similar with spatial statistics, the parameterization of these covariance function is one of the most difficult challenges in spatio-temporal statistics. 4.2.1 Spatio-Temporal Covariance Functions As mentioned above, we need to know covariance function, i.e., \\(\\mathbf{C}_z, \\mathbf{c}_0\\), to get the S-T kriging predictors. However, not any matrix can be covariance function, which requires some conditions, i.e., non-negative-definite. The general spatio-temporal covariance function would be \\[\\begin{equation} c_*(s,s&#39; ; t,t&#39;) \\equiv Cov(Y(s;t), Y(s&#39;;t&#39;)) \\tag{11} \\end{equation}\\] In practice, we commonly assume second-order stationary; the random process is said to be second-order (or weakly) stationary if it has a constant expection and the covariance function would be expressed in terms of only spatioal and temporal lags not others. That is, \\[\\begin{equation} c_*(s,s&#39; ; t,t&#39;) = c(s&#39;-s; t&#39;-t) = c(h; \\tau) \\tag{12} \\end{equation}\\] The second-order stationary assumption gives us many advantages, which allows some easy expression of covariance function, thus easy interpretation. Then, how do we obtain valid stionary spatio-temporal covariance functions? Separable (in Space and Time) Covariance Functions Separable covariance function have often been used because it is simple, easy to interpret but guarantee validity. It is given by \\[\\begin{equation} c(h; \\tau) \\equiv c^{(s)}(h) \\cdot c^{(t)}(\\tau) \\end{equation}\\] There are lots of valid spatial covariance function and valid temporal covariance function. For example, the most widely used one is Matern covariance function, which is \\[\\begin{equation} c^{(s)}(h) = \\sigma^2_s exp \\{ - \\dfrac{||h||}{a_s} \\} \\end{equation}\\] where \\(\\sigma^2_s\\) is the variance parameter and \\(a_s\\) is the spatial dependence parameter. More \\(a_s\\) results in smaller value in exponential, meaning large value of \\(\\mathbf{c}(h)\\). Moreover, separable covariance function reduce computation time, becuase the spatio-temporal covariance function can be decomposed into kronecker product. Because we have to compute inverse matrix of spatio-temporal covariance function, the computation could be enormously. However, by using the property of kronecker, we could reduce lots of computation. However, separable covariance function assumes that the temporal evolution of the process at a given spatial location does not depend directly on the process’ temporal evolution at other locations, which is very seldom for real-word. So we should consider other covariance functions, which will be discussed next. Sums-andProducts Formulation Using the property that sums of non-negative-definite function is also non-negative-definite, we present another form of covariance function, \\[\\begin{equation} c(h;\\tau) \\equiv p c^{(s)}_1 (h) \\cdot c^{(t)}_1(\\tau) + q c^{(s)}_2(h) + r c^{(t)}_2(\\tau) \\tag{13} \\end{equation}\\] Note that covariance function in (13) assumes fully symmetric spatio-temporal covariance function which is not realistic in real world. Construction via a Spectral Representation Developed by Cressie and Huang, they express the covariance function as \\[\\begin{equation} c(h;\\tau) = \\sigma^2 exp\\{ -b^2 ||h||^2 / (a^2 \\tau^2 + 1) \\} / (a^2 \\tau^2 + 1)^{d/2} \\tag{14} \\end{equation}\\] Stochastic Partial Differential Equation (SPDE) Approach Seldom directly appropriate for physical processes but may still provide good fits to data. 4.2.2 Spatio-Temporal Semivariograms In spatial statistics, it is common to consider dependence through the spatial variogram. Likewise, we can consider spatio-temporal variogram as \\[\\begin{equation} Var(Y(s;t) - Y(s&#39;;t&#39;)) \\equiv 2 \\gamma(s,s&#39;; t,t&#39;) \\tag{15} \\end{equation}\\] Similar with the definition of stationary spatial process, the stationary version of spatio-temporal variogram is denoted by \\(2\\gamma(h;\\tau)\\). Although variogram is widely used as exploratory analysis to check spatial dependence, this is not the common case in spatio-temporal statistical modeling. The main reason for this is that most of the real-world processes are best described as local second-order stationarity. If only local staionarity is expected and modeled, the extra generality given by the variogram is not needed. If this extra generality is included in the model without caution, some undesirable situations may occurr, which results in un-optimal variogram-based kriging predictor. However, when using covariance-based kriging predictor, we do not have to worry about such things. 4.2.3 Gaussian Spatio-Temporal Model Estimation In spatial statistics, it is common to fit covariance function (or semivariograms) directly to the empirical estimates, e.g., least squares or weighted least squares. However, in spatio-temporal statistics, we consider fully parameterized covariance structure and infer the parameters through likelikhood-based methods or bayesian methods. Likelihood Estimation We consider following likelihood, \\[\\begin{equation} L(\\beta, \\theta, Z) \\propto |\\mathbf{C}_z (\\theta) |^{-1/2} exp \\{ -\\dfrac12(Z - X\\beta)&#39; \\mathbf{C}_z(\\theta)^{-1}(Z-X\\beta) \\} \\tag{16} \\end{equation}\\] We maximize (16) w.r.t. \\(\\beta,\\theta\\). To reduce computation, we profile \\(\\beta\\) with GLS estimator and find the mle of \\(\\theta\\). Finally, we get the mle of \\(\\beta\\). Estimator using this method is called empirical best linear unbiased predictor (EBLUP). Bayesian Inference Instead fixing \\(\\theta, \\beta\\), we set the prior on them in bayesian statistics. Because the closed form of posterior distribution is often not possible, we get the numerical evaluation of it. \\end{equation} 4.3 Random-Effects Parameterizations Models including random-effects can take conditional or marginal approach depending on the purpose of analysis. If one specifies intercepts and slopes as random effects, inference at the subjevt (individual) level can be considered. In contrast, if the ficed treatment effects \\(\\beta\\) is the main interest, one might consider the marginal distribution of the responses where individual random effects have been integrated out. In spatial or spatio-temporal modeling, same considerations apply except that the random effects account for spatial or spatio-temporal effect. This is important in that it allows us to build spatio-temporal depdence conditionally, which allows us to build spatio-temporal dependence conditionally and some computational advantages. [See the technical note 4.3 in textbook for more details.] 4.4 Basis-Function Representations We talked about basis function at chapter 1, 3. Similar to random effects, coefficient of basis functions can be fixed effect or andom effect. If we consider random effect, it can be spatial, temporal or spatio-temporal basis functions. We will see those in the following subsections. 4.4.1 Random Effects with Spatio-Temporal Basis Functionss Suppose the observation and process model in (3). We resrite the process model in terms of fixed and random effects, i.e., \\[\\begin{equation} Y(s;t) = \\mathbf{x}(s;t)&#39; \\beta + \\eta(s;t) = \\mathbf{x}(s;t)&#39; \\beta + \\sum^{n_\\alpha}_{i=1} \\phi_i(s;t)\\alpha_i + \\nu(s;t) \\tag{17} \\end{equation}\\] Note that basis functions in (17) are functions of \\(s,t\\); \\(\\phi(s;t)\\). The corresponding coefficient \\(\\alpha_i\\) is random effects. \\(\\nu(s;t)\\) is sometimes needed to represent small-scale spatio-temporal random effects not captured by the basis functions. That is, the spatio-temporal random process, \\(\\eta(s;t)\\) is decomposed into a linear combination of \\(n_\\alpha\\) random effects and a residual error term. Suppose \\(\\alpha \\sim Gau(0, \\mathbf{C}_\\alpha)\\), where \\(\\alpha \\equiv (\\alpha_1, \\cdots, \\alpha_{n_\\alpha})\\). In vector notation, the spatio-temporal process model can be represented as \\[\\begin{equation} \\mathbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\Phi \\alpha} + \\mathbb{\\nu} \\tag{18} \\end{equation}\\] From technical note 4.3, the marginal distribution of \\(\\mathbf{Y}\\) is given by \\(\\mathbf{Y} \\sim Gau(\\mathbf{X}\\beta, \\mathbf{\\Phi C_\\alpha \\Phi &#39;} + \\mathbf{C_\\nu})\\). Benefit ot this approach is that we focus on fixed number of \\(n_\\alpha\\) random effects, which we set far less thatn \\(n_y\\), original dimension, for computational gain. Moreover, the covariance matrix of \\(\\mathbf{Z}\\), the observation process, can be expressed as \\(\\mathbf{C_z} = \\mathbf{\\Phi C_\\alpha \\Phi &#39;} + \\mathbf{C_\\nu} + \\mathbf{C}_\\epsilon\\), on which we can apply Sherman-Morrison-Woodbery matrix identities. Therefore, we can get great computation efficienccy. Choices of basis functions can be tricky because there are many options for spatio-temporal basis functions; 1) fixed or parameterized basis functions, 2) local or global basis functions, 3) reduced-rank, complete, or over-complete bases, 4) basis functions with expansion coefficients possibly indexed by space, time or space-time. Moreover, the thoice is affected by the presence and type of residual structure and the distribution of the random effects, so it is not easy to use basis functions. One simple attempt could be tensor-product basis functions, where we define the spatio-temporal basis function as the product of a spatial basis function and a temporal basis function. Example: Fixed Rank Kriging 4.4.2 Random Effects with Spatial Basis Functions. We can also consider the basis functions of space only and random coefficients indexed by time. \\[\\begin{equation} Y(s;t) = \\mathbf{x}(s;t)&#39; \\beta + \\eta(s;t) = \\mathbf{x}(s;t)&#39; \\beta + \\sum^{n_\\alpha}_{i=1} \\phi_i(s)\\alpha_i(t_j) + \\nu(s;t) \\tag{19} \\end{equation}\\] For spatial only basis model specified in (19), there are two options for \\(\\alpha\\). One is independent assumption, i.e. \\(\\alpha_{t_1}, \\alpha_{t_2}, \\cdots\\), which results in separable spatio-temporal dependence structure. The other is dependent assumption to model complex spatio-temporal dependence structure which will be discuseed in Chapter 5. 4.4.3 Random Effects with Temporal Basis Functions Finally, we can also express spatio-temporal random process in terms of temporal basis functions and spatially indexed random effects. \\[\\begin{equation} Y(s;t) = \\mathbf{x}(s;t)&#39; \\beta + \\eta(s;t) = \\mathbf{x}(s;t)&#39; \\beta + \\sum^{n_\\alpha}_{i=1} \\phi_i(t)\\alpha_i(s) + \\nu(s;t) \\tag{20} \\end{equation}\\] Model (20) is not as common as the model (18). However, the temporal basis functions are increasinly being used to model non-stationary-in-time processes that vary across space. Example Using Temporal Basis Functions 4.4.4 Confounding of Fixed Effects and Random Effects Consider general mixed-effects representation given in (18). The design matrix \\(\\mathbf{X}\\) is a function of space-time and matrix of basis function \\(\\mathbf{\\Phi}\\) can also be a function of space-time. That is, there is a concern that there is a potential confounding associated with the random effects. For this reason, many people choose basis functions in \\(\\mathbf{\\Phi}\\) orthogonal to the columns space of \\(\\mathbf{X}\\) to avoid this confounding. However, if one aims to predict the hidden process, this is not a problem at all. "]
]
